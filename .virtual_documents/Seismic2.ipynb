import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For machine learning
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder


import pandas as pd

# Load the dataset
file_path = "earthquake_1995-2023.csv"  # Replace with the actual file path if needed
df = pd.read_csv(file_path)

# Display the first few rows of the dataset
print(df.head())

# Check basic information about the dataset
print(df.info())


df.dropna(inplace=True)


print("Missing values:\n", df.isnull().sum())


print(df['location'].nunique())


df = pd.get_dummies(df, columns=['location'], drop_first=True)


df = pd.read_csv('earthquake_1995-2023.csv')


df.drop(['title', 'continent', 'country'], axis=1, inplace=True)


df = pd.get_dummies(df, columns=['magType', 'alert', 'net'], drop_first=True)


print(df.dtypes)


bool_cols = df.select_dtypes(include='bool').columns
df[bool_cols] = df[bool_cols].astype(int)


print(df.dtypes)



df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce',dayfirst=True)
df['year'] = df['date_time'].dt.year
df['month'] = df['date_time'].dt.month
df['day'] = df['date_time'].dt.day
df['hour'] = df['date_time'].dt.hour
df.drop('date_time', axis=1, inplace=True)

# 2. Encode 'location' 
df = pd.get_dummies(df, columns=['location'], drop_first=True)  # or use LabelEncoder for compact form

# 3. Handle any NaN
df.fillna(0, inplace=True)


import joblib

# After preprocessing and feature engineering, before train-test split:
joblib.dump(X.columns.tolist(), 'model_columns.pkl')


X = df.drop('magnitude', axis=1)  # All columns except magnitude
y = df['magnitude']


from sklearn.model_selection import train_test_split

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.ensemble import RandomForestRegressor

# Initialize model
rfr = RandomForestRegressor(n_estimators=100, random_state=42)

# Train model
rfr.fit(X_train, y_train)


y_pred = rfr.predict(X_test)



from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R² Score: {r2:.4f}")


import matplotlib.pyplot as plt

# Feature importance
importances = rfr.feature_importances_
feature_names = X.columns

# Sort importances
indices = np.argsort(importances)[::-1]

# Plot
plt.figure(figsize=(12, 8))
plt.title("Feature Importance")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()


import matplotlib.pyplot as plt
import numpy as np

# Get feature importances and names
importances = rfr.feature_importances_
feature_names = X.columns

# Sort features by importance
indices = np.argsort(importances)[::-1]  # descending order

# Pick top 20
top_n = 20
top_indices = indices[:top_n]

# Plot
plt.figure(figsize=(14, 6))
plt.title(f"Top {top_n} Feature Importances")
plt.bar(range(top_n), importances[top_indices], color="g", align="center")
plt.xticks(range(top_n), [feature_names[i] for i in top_indices], rotation=45, ha='right')
plt.tight_layout()
plt.show()


import pandas as pd

# Convert to DataFrame
fi_df = pd.DataFrame({'feature': X.columns, 'importance': importances})

# Aggregate importance by feature prefix
fi_df['group'] = fi_df['feature'].apply(lambda x: x.split('_')[0] if '_' in x else x)
grouped_fi = fi_df.groupby('group')['importance'].sum().sort_values(ascending=False)

# Plot grouped importances
plt.figure(figsize=(10, 6))
grouped_fi.plot(kind='bar')
plt.title('Aggregated Feature Importances')
plt.ylabel('Total Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()


from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Grid search
grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42), 
                           param_grid=param_grid,
                           cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best model
best_rfr = grid_search.best_estimator_
print("Best parameters found: ", grid_search.best_params_)



# Predict again
y_pred_best = best_rfr.predict(X_test)

# Metrics
mse_best = mean_squared_error(y_test, y_pred_best)
rmse_best = np.sqrt(mse_best)
r2_best = r2_score(y_test, y_pred_best)

print(f"Best MSE: {mse_best:.4f}")
print(f"Best RMSE: {rmse_best:.4f}")
print(f"Best R²: {r2_best:.4f}")


import joblib
# Save
joblib.dump(rfr, 'earthquake_magnitude_rfr.pkl')


import pandas as pd
import joblib

# Load model and feature columns
model = joblib.load('earthquake_magnitude_rfr.pkl')
columns = joblib.load('model_columns.pkl')  # Load exact feature columns

# Take user input
depth = float(input("Enter depth of earthquake: "))
sig = int(input("Enter significance (sig): "))
cdi = int(input("Enter Community Internet Intensity (cdi): "))
mmi = int(input("Enter Modified Mercalli Intensity (mmi): "))
location_input = input("Enter location: ")
net_input = input("Enter network: ")
mag_type_input = input("Enter magnitude type (e.g., mb, ml): ")

# Prepare input DataFrame with all columns initialized to zero
input_df = pd.DataFrame(columns=columns)
input_df.loc[0] = 0  # Initialize first row to zeros

# Set numeric features
input_df.loc[0, 'depth'] = depth
input_df.loc[0, 'sig'] = sig
input_df.loc[0, 'cdi'] = cdi
input_df.loc[0, 'mmi'] = mmi

# Handle categorical fields: ensure unseen values don't cause issues
for col_prefix, user_input in [('location_', location_input), ('net_', net_input), ('magType_', mag_type_input)]:
    matching_cols = [col for col in columns if col.startswith(col_prefix)]
    if matching_cols:  # Ensure at least one related column exists
        best_match = min(matching_cols, key=lambda col: abs(len(col) - len(user_input)))
        input_df.loc[0, best_match] = 1
    else:
        print(f"Warning: {col_prefix[:-1]} '{user_input}' not seen during training. Proceeding without it.")

# Predict
prediction = model.predict(input_df)[0]
print(f"\n✅ Predicted Earthquake Magnitude: {prediction:.2f}")





train_preds = best_rfr.predict(X_train)
test_preds = best_rfr.predict(X_test)

print("Train Predictions: ", train_preds[:10])  # Show first 10 predictions
print("Test Predictions: ", test_preds[:10])   # Show first 10 predictions



print("Training Data Sample:\n", X_train.head())
print("Target Variable Sample:\n", y_train.head())
print("Data Shape: ", X_train.shape, y_train.shape)



import pandas as pd
import numpy as np

feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': best_rfr.feature_importances_
})

print(feature_importance.sort_values(by="Importance", ascending=False))



print("Mean of y_train: ", np.mean(y_train))



print("Training Columns:", X_train.columns.tolist())
print("Test Columns:", X_test.columns.tolist())



print(set(X_train.columns) - set(X_test.columns))  # Features missing in test set
print(set(X_test.columns) - set(X_train.columns))  # Features missing in training set



low_importance_features = feature_importance[feature_importance["Importance"] == 0]["Feature"].tolist()
X_train = X_train.drop(columns=low_importance_features)
X_test = X_test.drop(columns=low_importance_features)



from sklearn.metrics import mean_absolute_error, r2_score

print("Train MAE:", mean_absolute_error(y_train, train_preds))
print("Test MAE:", mean_absolute_error(y_test, test_preds))

print("Train R² Score:", r2_score(y_train, train_preds))
print("Test R² Score:", r2_score(y_test, test_preds))



import numpy as np
print("Unique predictions in train:", np.unique(train_preds))
print("Unique predictions in test:", np.unique(test_preds))



top_features = feature_importance.sort_values(by="Importance", ascending=False)["Feature"].iloc[:10].tolist()
X_train_reduced = X_train[top_features]
X_test_reduced = X_test[top_features]

# Train model again
best_rfr.fit(X_train_reduced, y_train)
train_preds_reduced = best_rfr.predict(X_train_reduced)
test_preds_reduced = best_rfr.predict(X_test_reduced)

print("Test Predictions (Reduced Features):", test_preds_reduced[:10])



import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np



plt.figure(figsize=(8, 5))  # Set figure size
sns.histplot(y_train, bins=20, kde=True, color='royalblue')  # Histogram with KDE curve
plt.xlabel("Earthquake Magnitude")
plt.ylabel("Frequency")
plt.title("Distribution of Earthquake Magnitudes in Training Data")
plt.grid(True)
plt.show()



print("Min value in y_train:", np.min(y_train))
print("Max value in y_train:", np.max(y_train))



print("Min and Max values in the original dataset:")
print("Min value in full dataset:", df["magnitude"].min())
print("Max value in full dataset:", df["magnitude"].max())




